{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTaB+dRFf5TjQr0Bb/H+0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unikevde/DeberSemana5/blob/master/EJERCICIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l2fl6O9ZkiAO"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# echo \"FirstName LastName\" > user.txt\n",
        "# echo `date` >> user.txt\n",
        "# cat user.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# hdfs dfs -put user.txt\n",
        "# hdfs dfs -ls -R /user/navaro_p/"
      ],
      "metadata": {
        "id": "tbLsPCapot_C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# hdfs dfs -cat user.txt"
      ],
      "metadata": {
        "id": "bjE8EprIowMB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "# input comes from standard input\n",
        "for line in sys.stdin:\n",
        "    line = line.strip().lower() # remove leading and trailing whitespace\n",
        "    line = line.replace(\".\", \" \")   # strip punctuation\n",
        "    for word in line.split(): # split the line into words\n",
        "        # write the results to standard output;\n",
        "        # what we output here will be the input for the\n",
        "        # Reduce step, i.e. the input for reducer.py\n",
        "        # tab-delimited; the trivial word count is 1\n",
        "        print (f'{word}\\t 1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox---7F_o-4x",
        "outputId": "754a4990-d643-4b09-f855-1cc6b35d5683"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# chmod +x mapper.py\n",
        "# cat sample01.txt | ./mapper.py | sort"
      ],
      "metadata": {
        "id": "Bx6tJr8LpE9R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file reducer.py\n",
        "#!/usr/bin/env python\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "for line in sys.stdin:\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    word, count = line.split('\\t', 1)\n",
        "\n",
        "    # convert count (currently a string) to int\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # count was not a number, so silently\n",
        "        # ignore/discard this line\n",
        "        continue\n",
        "\n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    # by key (here: word) before it is passed to the reducer\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            # write result to sys.stdout\n",
        "            print (f'{current_count}\\t{current_word}')\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "    print (f'{current_count}\\t{current_word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX3iWJlapJw6",
        "outputId": "77c877c8-908f-494d-a884-ab91d5688e46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# chmod +x reducer.py\n",
        "# ./mapper.py < sample01.txt | sort | ./reducer.py | sort"
      ],
      "metadata": {
        "id": "L57GRTXRpNei"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Makefile\n",
        "\n",
        "HADOOP_VERSION=2.7.6\n",
        "HADOOP_HOME=/export/hadoop-${HADOOP_VERSION}\n",
        "HADOOP_TOOLS=${HADOOP_HOME}/share/hadoop/tools/lib\n",
        "HDFS_DIR=/user/${USER}\n",
        "\n",
        "SAMPLES = sample01.txt sample02.txt sample03.txt sample04.txt\n",
        "\n",
        "copy_to_hdfs: ${SAMPLES}\n",
        "\thdfs dfs -mkdir -p ${HDFS_DIR}/input\n",
        "\thdfs dfs -put $^ ${HDFS_DIR}/input\n",
        "\n",
        "run_with_hadoop:\n",
        "\thadoop jar ${HADOOP_TOOLS}/hadoop-streaming-${HADOOP_VERSION}.jar \\\n",
        "    -file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
        "    -file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
        "    -input ${HDFS_DIR}/input/*.txt -output ${HDFS_DIR}/output-hadoop\n",
        "\n",
        "run_with_yarn:\n",
        "\tyarn jar ${HADOOP_TOOLS}/hadoop-streaming-${HADOOP_VERSION}.jar \\\n",
        "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
        "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
        "\t-input ${HDFS_DIR}/input/*.txt -output ${HDFS_DIR}/output-yarn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEAqp6RUpTGK",
        "outputId": "00d97aff-4f84-4089-dc7a-64ec9f0a8a61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# hdfs dfs -rm -r input  # remove input directory\n",
        "# make copy_to_hdfs # copy sample files to hdfs\n",
        "# hdfs dfs -ls input # list files on hdfs"
      ],
      "metadata": {
        "id": "CrCXHj3VpWZK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# hdfs dfs -rm -r -f output-hadoop # Remove output directory on hdfs\n",
        "# make run_with_hadoop  # Run the hadoop streaming map reduce process\n",
        "# hdfs dfs -cat output-hadoop/*  # Display results"
      ],
      "metadata": {
        "id": "iM8fHXfNpZB6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0D1YEyJlpdF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}